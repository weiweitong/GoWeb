# Process100G
Process 100G data with 1G memory. 
用1G内存处理100G的大文件，并找出其中出现次数最多的前100个元素。

首先使用channel与go协程并发加速，但不会造成写冲突地去生成100G的单个大文件。

然后使用HashMap，切分大文件成若干个小于1G的文件。

再使用100个元素的最小堆做一个出现次数的排序。

同时考虑到了最优的时间复杂度，
并且对IO次数也有优化。

并完成对应的测试用例，
以及benchmark的开发
